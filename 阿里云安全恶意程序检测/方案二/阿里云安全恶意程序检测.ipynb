{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9263426-de3f-4181-a52e-2c51dadd4f4e",
   "metadata": {},
   "source": [
    "所有的数据太大了，我的机子带不起来，就随机抽样20\\%的进行实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29527250-c734-4b0a-93da-76a94c107cc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def sample_csv(file_path, save_path, sample_rate=0.2, chunk_size=1000000):\n",
    "    sampled_data = []\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        sample_size = int(len(chunk) * sample_rate)\n",
    "        chunk_sample = chunk.sample(n=sample_size)\n",
    "        sampled_data.append(chunk_sample)\n",
    "    \n",
    "    # 将所有抽样的数据合并为一个DataFrame\n",
    "    sampled_df = pd.concat(sampled_data, ignore_index=True)\n",
    "    \n",
    "    # 保存抽样后的数据到新的CSV文件\n",
    "    sampled_df.to_csv(save_path, index=False)\n",
    "\n",
    "# 调用函数\n",
    "file_path = r\"D:\\jupyter-home\\Data\\security_train.csv\"  # 替换为你的CSV文件路径\n",
    "save_path = r\"D:\\jupyter-home\\Data_small\\security_train.csv\"  # 替换为你希望保存抽样数据的路径\n",
    "sample_csv(file_path, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f373bd37-ea62-4cf2-b7dc-980d9e74785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 读txt文件\n",
    "def read_data(path):\n",
    "    content = []\n",
    "    file = open(path, 'r', encoding='UTF-8')\n",
    "    for line in file:\n",
    "        content.append(line.strip())\n",
    "    file.close()\n",
    "    return content\n",
    "\n",
    "\n",
    "# 写txt文件(追加)\n",
    "def write_data(path, data):\n",
    "    fos = open(path, \"a\", encoding=\"utf-8\")\n",
    "    fos.write(data + '\\n')\n",
    "    fos.close()\n",
    "\n",
    "\n",
    "# 读csv文件\n",
    "def read_data_csv(path):\n",
    "    birth_data = []\n",
    "    with open(path, encoding=\"utf-8\") as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)  # 使用csv.reader读取csvfile中的文件\n",
    "        birth_header = next(csv_reader)  # 读取第一行每一列的标题\n",
    "        for row in csv_reader:  # 将csv 文件中的数据保存到birth_data中\n",
    "            birth_data.append(row)\n",
    "    return birth_data\n",
    "\n",
    "\n",
    "# 写入csv文件\n",
    "def write_data_csv(path, data):\n",
    "    with open(path, \"a\", encoding=\"utf-8\", newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        # 先写入columns_name\n",
    "        # writer.writerow([\"content_id\", \"train_subject\", \"sentiment_value\", \"sentiment_word\"])\n",
    "        # 写入多行用writerows\n",
    "        temp = data.split(',')\n",
    "        # writer.writerows([[temp[0], temp[1], temp[2], temp[3]]])\n",
    "\n",
    "        temp_list = []\n",
    "        for i in range(len(temp)):\n",
    "            temp_list.append(temp[i])\n",
    "        writer.writerows([temp_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c272f51-e5a9-4d01-857c-68cc1dd4b7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加入特征\n",
    "def add_feature(Data):\n",
    "    train_df = pd.read_csv(f\"./{Data}/security_train.csv\")  # file_id,label,api,tid,index\n",
    "    train_df['return_value'] = 0\n",
    "    train_df.to_csv(f\"./{Data}/security_train2.csv\", index=False, sep=',')\n",
    "\n",
    "    test_df = pd.read_csv(f\"./{Data}/security_test.csv\")  # file_id,label,api,tid,index\n",
    "    test_df['return_value'] = 0\n",
    "    test_df.to_csv(f\"./{Data}/security_test2.csv\", index=False, sep=',')\n",
    "add_feature(\"Data_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5238cf12-a62c-49bf-a039-8cb5e213a740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   file_id  label                     api   tid  index\n",
      "0      143      7              LdrLoadDll  4768    128\n",
      "1       98      5             RegCloseKey  2720   3795\n",
      "2      143      7  LdrGetProcedureAddress  3092     54\n",
      "3       46      6          Process32NextW  2432   2866\n",
      "4       64      5          Process32NextW  2328   2361\n",
      "label\n",
      "[0]    4958\n",
      "[5]    4042\n",
      "[7]    1487\n",
      "[2]    1193\n",
      "[3]     820\n",
      "[6]     514\n",
      "[1]     500\n",
      "[4]     100\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 数据分析\n",
    "def data_analysis(Data):\n",
    "    # data_df=pd.read_csv(\"train.csv\",nrows=100000000)\n",
    "    data_df = pd.read_csv(f\"./{Data}/security_train.csv\")\n",
    "    # data_df.head()\n",
    "    print(data_df.head())\n",
    "    label_df = data_df.groupby(['file_id', 'label'])['label'].unique()\n",
    "    label_class = label_df.value_counts()\n",
    "    print(label_class)\n",
    "    dict = {0: \"Normal\", 1: \"Extortion Virus\", 2: \"Mining Program\", 3: \"DDoS Trojan Horse\", 4: \"Worm Virus\",\n",
    "            5: \"Infectious Virus\", 6: \"Backdoor Program\", 7: \"Trojan Horse Program\"}\n",
    "\n",
    "data_analysis(\"Data_small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22979d-a78d-4a43-958f-8e4448de3fc6",
   "metadata": {},
   "source": [
    "### 2.2 特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3dcc05e-e372-4b71-bfcd-f97f9a1785d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15857675, 4) (17961338, 5)\n"
     ]
    }
   ],
   "source": [
    "# 查看数据\n",
    "def check_csv(Data):\n",
    "    data_test_df = pd.read_csv(f\"./{Data}/security_test.csv\")  # file_id,label,api,tid,index\n",
    "    data_train_df = pd.read_csv(f\"./{Data}/security_train.csv\")  # file_id,label,api,tid,index\n",
    "    print(data_test_df.shape, data_train_df.shape)\n",
    "\n",
    "check_csv(\"Data_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd82841f-36d9-43b4-bd01-f512de61a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# 对训练集的数据预处理\n",
    "def train_data_preprocess(Data):\n",
    "    # 数据读取\n",
    "    print('正在读取训练集')\n",
    "    train = pd.read_csv(f'./{Data}/security_train.csv')\n",
    "    train['return_value'] = 0  \n",
    "\n",
    "    print('正在提取训练集特征')\n",
    "    # 特征工程 & 验证结果(1-Gram)\n",
    "    train_data = train[['file_id', 'label']].drop_duplicates()\n",
    "    train_data.head()\n",
    "    train_data['label'].value_counts()\n",
    "\n",
    "    # 提取特征一：api\n",
    "    feature_list1 = ['count', 'nunique']\n",
    "    train_data = feature_extraction1(train, train_data, feature_list1)\n",
    "\n",
    "    # 提取特征二：tid\n",
    "    feature_list2 = ['count', 'nunique', 'max', 'min', 'median', 'std']\n",
    "    train_data = feature_extraction2(train, train_data, feature_list2)\n",
    "\n",
    "    # 提取特征三：index\n",
    "    feature_list3 = ['count', 'nunique', 'max', 'min', 'median', 'std']\n",
    "    train_data = feature_extraction3(train, train_data, feature_list3)\n",
    "\n",
    "    # 训练特征\n",
    "    train_features = [col for col in train_data.columns if col != 'label' and col != 'file_id']\n",
    "    train_label = 'label'\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(train_data[train_features], train_data[train_label].values,\n",
    "                                                        test_size=0.33)\n",
    "    gc.collect()\n",
    "\n",
    "    # 特征扩充：单个特征间的组合\n",
    "    train, train_data_, combination_feature = feature_extension(train, train_data)\n",
    "\n",
    "    # 多特征提取\n",
    "    train, train_data_, combination_feature = feature_extension_multi(train, train_data_)\n",
    "\n",
    "    # 采用lgb训练\n",
    "    lgb_train(train_X, test_X, train_Y, test_Y, train, train_data_, train_features, combination_feature)\n",
    "\n",
    "\n",
    "# 对测试集的数据预处理\n",
    "def test_data_preprocess(Data):\n",
    "    # 生成test数据集特征\n",
    "    # 数据读取\n",
    "    print('正在读取测试集')\n",
    "    test = pd.read_csv(f'../{Data}/security_test.csv', nrows=1000)\n",
    "    test['return_value'] = 0  # 定义一个特征变量\n",
    "    print(test.shape)\n",
    "\n",
    "    print('正在提取训练集特征')\n",
    "    # 特征工程\n",
    "    test_data = test[['file_id']].drop_duplicates()\n",
    "    test_data.head()\n",
    "\n",
    "    # 提取特征一：api\n",
    "    feature_list1 = ['count', 'nunique']\n",
    "    test_data = feature_extraction1(test, test_data, feature_list1)\n",
    "\n",
    "    # 提取特征二：title_id\n",
    "    feature_list2 = ['count', 'nunique', 'max', 'min', 'median', 'std']\n",
    "    test_data = feature_extraction2(test, test_data, feature_list2)\n",
    "\n",
    "    # 提取特征三：index\n",
    "    feature_list3 = ['count', 'nunique', 'max', 'min', 'median', 'std']\n",
    "    test_data = feature_extraction3(test, test_data, feature_list3)\n",
    "\n",
    "    # 训练特征 & 标签\n",
    "    test, test_data_, combination_feature = feature_extension_test(test, test_data)\n",
    "    test_data, test_data_, combination_feature = feature_extension_multi_test(test, test_data, test_data_)\n",
    "    test_data_.to_csv(f'{Data}/feature/test_data.csv', index=None)\n",
    "    \n",
    "def feature_combination(data_merge, data_orig, combination_feature, col1=None, col2=None, opts=None):\n",
    "    for opt in opts:\n",
    "        # print(opt)\n",
    "        train_split = data_orig.groupby(['file_id', col1])[col2].agg(\n",
    "            {'fileid_' + col1 + '_' + col2 + '_' + str(opt): opt}).reset_index()\n",
    "\n",
    "        train_split_ = pd.pivot_table(train_split, values='fileid_' + col1 + '_' + col2 + '_' + str(opt),\n",
    "                                      index=['file_id'], columns=[col1])\n",
    "        new_cols = ['fileid_' + col1 + '_' + col2 + '_' + opt + '_' + str(col) for col in train_split_.columns]\n",
    "\n",
    "        combination_feature.append(new_cols)\n",
    "        train_split_.columns = new_cols\n",
    "\n",
    "        train_split_.reset_index(inplace=True)\n",
    "\n",
    "        data_merge = pd.merge(data_merge, train_split_, how='left', on='file_id')\n",
    "    return data_merge, combination_feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
